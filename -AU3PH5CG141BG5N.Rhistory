arima2<- Arima(serie_tiempo,order =c(0,1,0) )
arima3<- Arima(serie_tiempo,order =c(1,0,0) )
arima4<- Arima(serie_tiempo,order =c(2,2,1) )
arima5<- Arima(serie_tiempo,order =c(0,1,1) )
arima6<- Arima(serie_tiempo,order =c(2,2,0) )
arima7<- Arima(serie_tiempo,order =c(2,1,0) )
arima8<- Arima(serie_tiempo,order =c(3,2,2) )
arima9<- Arima(serie_tiempo,order =c(1,1,1) )
arima10<- Arima(serie_tiempo,order =c(1,1,2) )
arima11<- Arima(serie_tiempo,order =c(1,1,3) )
arima12<- Arima(serie_tiempo,order =c(0,1,2) )
arima13<- Arima(serie_tiempo,order =c(1,1,0))
AIC(arima1,arima2,arima3,arima4,arima5,arima6,arima7,arima8,arima9,arima10,arima11,arima12,arima13)
# EL MEJOR MODELO CON AIC ES EL MODELO 1a YA QUE TIENE EL MODELO MAS BAJO
BIC(arima1,arima2,arima3,arima4,arima5,arima6,arima7,arima8,arima9,arima10,arima11,arima12,arima13)
tsdiag(arima9)
Box.test(residuals(arima9),type ="Ljung-Box")
error=residuals(arima9)
plot (error)
pronostico_arima9 <- forecast::forecast(arima9 ,h=100)
pronostico_arima9
plot(pronostico_arima9)
pronostico_arima9 <- forecast::forecast(arima9 ,h=12)
pronostico_arima9
plot(pronostico_arima9)
pronostico_arima9 <- forecast(arima9 ,level=c(95),h=12)
pronostico_arima9
pronostico_arima9 <- forecast(arima9 ,level=c(95),h=50)
pronostico_arima9 <- forecast(arima9 ,level=c(95),h=50)
pronostico_arima9
modelo_arima_drift <- Arima(serie_tiempo, order=c(2,1,2), include.drift=TRUE)
summary(modelo_arima_drift)
# Pronóstico utilizando el modelo ARIMA con drift
pronostico_arima_drift <- forecast(modelo_arima_drift, h=50)
plot(pronostico_arima_drift)
modelo_arima_drift <- Arima(serie_tiempo, order=c(2,1,2), include.drift=TRUE)
summary(modelo_arima_drift)
modelo_arima_drift
# Pronóstico utilizando el modelo ARIMA con drift
pronostico_arima_drift <- forecast(modelo_arima_drift, h=50)
plot(pronostico_arima_drift)
modelo_arima_drift <- Arima(serie_tiempo, order=c(2,1,2), include.drift=TRUE)
summary(modelo_arima_drift)
modelo_arima_drift
# Pronóstico utilizando el modelo ARIMA con drift
pronostico_arima_drift <- forecast(modelo_arima_drift, h=50)
pronostico_arima_drift
plot(pronostico_arima_drift)
modelo_arima_drift <- Arima(serie_tiempo, order=c(2,1,2), include.drift=TRUE)
summary(modelo_arima_drift)
modelo_arima_drift
# Pronóstico utilizando el modelo ARIMA con drift
pronostico_arima_drift <- forecast(modelo_arima_drift, h=150)
pronostico_arima_drift
plot(pronostico_arima_drift)
modelo_arima_drift <- Arima(serie_tiempo, order=c(1,1,1), include.drift=TRUE)
summary(modelo_arima_drift)
modelo_arima_drift
# Pronóstico utilizando el modelo ARIMA con drift
pronostico_arima_drift <- forecast(modelo_arima_drift, h=150)
pronostico_arima_drift
plot(pronostico_arima_drift)
tsdiag(modelo_arima_drift)
Box.test(residuals(arima9),type ="Ljung-Box")
Box.test(residuals(modelo_arima_drift),type ="Ljung-Box")
error=residuals(arima9)
plot (error)
error=residuals(modelo_arima_drift)
plot (error)
Box.test(residuals(modelo_arima_drift),type ="Ljung-Box")
error=residuals(modelo_arima_drift)
plot (error)
library(forecast)
# Usar auto.arima con drift
modelo_auto_con_drift <- auto.arima(serie_tiempo, d=1, D=0, seasonal=FALSE, allowdrift=TRUE)
# Resumen del modelo
summary(modelo_auto_con_drift)
# Pronóstico utilizando el modelo auto.arima con drift
pronostico_auto_con_drift <- forecast(modelo_auto_con_drift, h=30)  # Pronosticar 30 días hacia adelante
plot(pronostico_auto_con_drift)
# Pronóstico utilizando el modelo auto.arima con drift
pronostico_auto_con_drift <- forecast(modelo_auto_con_drift, h=30)  # Pronosticar 30 días hacia adelante
pronostico_auto_con_drift
plot(pronostico_auto_con_drift)
tamanio <-length(data)
tamanio
tamanio <-length(data)
data
data_sitio <- read.csv("data_sitio.csv", sep = ";")
data_sitio$M_FECHA_DIA <- as.Date(data_sitio$M_FECHA_DIA)
data_sitio <- data_sitio[order(data_sitio$M_FECHA_DIA), ]
data <- data_sitio[, c("M_FECHA_DIA", "PAYLOAD_4G")]
tamanio <-length(data)
data
tamanio <-length(data)
tamanio
set.seed(123)
train_size <- floor(0.7 * nrow(data))
train <- data[1:train_size, ]
test <- data[(train_size + 1):nrow(data_weekly), ]
set.seed(123)
train_size <- floor(0.7 * nrow(data))
train <- data[1:train_size, ]
test <- data[(train_size + 1):nrow(data), ]
train
library(zoo)
library(quantmod)
y <- as.zoo(S)
# Normalizar los datos
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Normalizar los datos
train_normalized <- train %>%
mutate(PAYLOAD_4G = normalize(PAYLOAD_4G))
set.seed(123)
train_size <- floor(0.7 * nrow(data))
train <- data[1:train_size, ]
test <- data[(train_size + 1):nrow(data), ]
train
# Normalizar los datos
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Normalizar los datos
train_normalized <- train %>%
mutate(PAYLOAD_4G = normalize(PAYLOAD_4G))
library(fpp2)
data_sitio <- read.csv("data_sitio.csv", sep = ";")
data_sitio$M_FECHA_DIA <- as.Date(data_sitio$M_FECHA_DIA)
data_sitio <- data_sitio[order(data_sitio$M_FECHA_DIA), ]
data <- data_sitio[, c("M_FECHA_DIA", "PAYLOAD_4G")]
tamanio <-length(data)
tamanio
# Normalizar los datos
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Normalizar los datos
train_normalized <- train %>%
mutate(PAYLOAD_4G = normalize(PAYLOAD_4G))
set.seed(123)
train_size <- floor(0.7 * nrow(data))
train <- data[1:train_size, ]
test <- data[(train_size + 1):nrow(data), ]
train
# Normalizar los datos
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Normalizar los datos
train_normalized <- train %>%
mutate(PAYLOAD_4G = normalize(PAYLOAD_4G))
# Normalizar los datos
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Normalizar los datos
train_normalized <- train %>%
mutate(PAYLOAD_4G = normalize(PAYLOAD_4G))
set.seed(123)
train_size <- floor(0.7 * nrow(data))
train <- data[1:train_size, ]
test <- data[(train_size + 1):nrow(data), ]
train
# Normalizar los datos
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Normalizar los datos
train_normalized <- train %>%
mutate(PAYLOAD_4G = normalize(PAYLOAD_4G))
library(fpp2)
install.packages("fpp2")
library(fpp2)
library(readxl)
library(forecast)# Contiene el modelo ARIMA
library(tseries) #Para series de tiempo
library(TSA)     #Para series de tiempo
library(ggplot2) #Para hacer gráficos
library(dplyr)   #Para la manipulación de datos (filtrar, seleccionar, agregar, transformar)
library(stats)   #Se usa para diversas pruebas estadísticas (medias,varianza, arima,etc)
library(seasonal)#Para calcular la serie ajustada de estacionalidad
install.packages("seasonal")
library(fpp2)
library(readxl)
library(forecast)# Contiene el modelo ARIMA
library(tseries) #Para series de tiempo
library(TSA)     #Para series de tiempo
library(ggplot2) #Para hacer gráficos
library(dplyr)   #Para la manipulación de datos (filtrar, seleccionar, agregar, transformar)
library(stats)   #Se usa para diversas pruebas estadísticas (medias,varianza, arima,etc)
library(seasonal)#Para calcular la serie ajustada de estacionalidad
library(zoo)     #Para calcular la serie ajustada de estacionalidad
library(tidyverse)
library(lubridate)
library(tidyverse)
library(urca)
library(fpp2)
library(readxl)
library(forecast)# Contiene el modelo ARIMA
library(tseries) #Para series de tiempo
library(TSA)     #Para series de tiempo
library(ggplot2) #Para hacer gráficos
library(dplyr)   #Para la manipulación de datos (filtrar, seleccionar, agregar, transformar)
library(stats)   #Se usa para diversas pruebas estadísticas (medias,varianza, arima,etc)
library(seasonal)#Para calcular la serie ajustada de estacionalidad
library(zoo)     #Para calcular la serie ajustada de estacionalidad
library(tidyverse)
library(lubridate)
library(tidyverse)
library(urca)
data_sitio <- read.csv("data_sitio.csv", sep = ";")
data_sitio$M_FECHA_DIA <- as.Date(data_sitio$M_FECHA_DIA)
data_sitio <- data_sitio[order(data_sitio$M_FECHA_DIA), ]
data <- data_sitio[, c("M_FECHA_DIA", "PAYLOAD_4G")]
tamanio <-length(data)
tamanio
set.seed(123)
train_size <- floor(0.7 * nrow(data))
train <- data[1:train_size, ]
test <- data[(train_size + 1):nrow(data), ]
train
# Normalizar los datos
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Normalizar los datos
train_normalized <- train %>%
mutate(PAYLOAD_4G = normalize(PAYLOAD_4G))
test_normalized <- test %>%
mutate(PAYLOAD_4G = normalize(PAYLOAD_4G))
library(zoo)
library(quantmod)
y <- as.zoo(train_normalized)
library(zoo)
library(quantmod)
y <- as.zoo(train_normalized)
library(zoo)
library(quantmod)
y <- as.zoo(train_normalized)
library(zoo)
library(quantmod)
y <- as.zoo(train_normalized)
data
# Leer el archivo CSV
data_sitio <- read.csv("data_sitio.csv", sep = ";")
# Convertir la columna M_FECHA_DIA a tipo Date
data_sitio$M_FECHA_DIA <- as.Date(data_sitio$M_FECHA_DIA)
# Ordenar los datos por fecha
data_sitio <- data_sitio[order(data_sitio$M_FECHA_DIA), ]
# Seleccionar las columnas relevantes
data <- data_sitio[, c("M_FECHA_DIA", "PAYLOAD_4G")]
# Crear un objeto de serie temporal
# Asumiendo que los datos son diarios y empiezan en la primera fecha del dataset
data_ts <- ts(data$PAYLOAD_4G, start = c(as.numeric(format(min(data$M_FECHA_DIA), "%Y")), as.numeric(format(min(data$M_FECHA_DIA), "%j"))), frequency = 365)
# Mostrar el objeto de serie temporal
print(data_ts)
Z <- as.ts(data_ts,F)
S <- (Z-min(Z))/(max(Z)-min(Z))
plot(S)
tamano_total <- length(S)
tamano_total
#Ahora dividiremos los conjuntos de entrenamiento en un 75% y prueba en un 25% respectivamente.
tamano_train <- round(tamano_total*0.75, digits = 0)
train <- 0:(tamano_train-1)
train
test <- (tamano_train):tamano_total
test
#Ahora crearemos un dataframe con n columnas, cada una de las cuales adelantara
#un valor de la serie en el futuro, a través de una variable tipo zoo, equivalente al #periodo de retardo de la serie.
library(zoo)
library(quantmod)
y <- as.zoo(S)
x1 <- Lag(y, k = 1)
x2 <- Lag(y, k = 2)
x3 <- Lag(y, k = 3)
x4 <- Lag(y, k = 4)
x5 <- Lag(y, k = 5)
x6 <- Lag(y, k = 6)
x7 <- Lag(y, k = 7)
x8 <- Lag(y, k = 8)
x9 <- Lag(y, k = 9)
x10 <- Lag(y, k = 10)
x11 <- Lag(y, k = 11)
x12 <- Lag(y, k = 12)
slogN <- cbind(y,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12)
#A continuacion eliminaremos los valores NA producidos al desplazar la serie:
slogN1 <- slogN[-(1:12),]
DT::datatable(slogN1)
#Luego definimos los valores de entrada y salida de la red neuronal:
inputs <- slogN1[,2:13]
outputs <- slogN1[,1]
library(RSNNS)
set.seed(42)
fit<-elman(inputs[train],outputs[train],size=5,learnFuncParams=c(0.1),
maxit=10000)
install.packages("RSNNS")
install.packages("RSNNS")
install.packages("RSNNS")
install.packages("RSNNS")
library(RSNNS)
set.seed(42)
fit<-elman(inputs[train],outputs[train],size=5,learnFuncParams=c(0.1),
maxit=10000)
install.packages("quantmod")
library(RSNNS)
set.seed(42)
fit<-elman(inputs[train],outputs[train],size=5,learnFuncParams=c(0.1),
maxit=10000)
library(RSNNS)
set.seed(42)
fit<-elman(inputs[train],outputs[train],size=5,learnFuncParams=c(0.1),
maxit=10000)
#En la gráfica siguiente vemos como evoluciona el error de la red con el numero de iteraciones para los parámetros expuestos.
plotIterativeError(fit, main = "Iterative Error for 5 Neuron")
#Ahora realizamos la predicción con el resto de los términos de la serie que son los datos #seleccionados para test, pasamos pues una vez entrenada a probarla y a representarla #graficamente para ver el ajuste del modelo.
y <- as.vector(outputs[-test])
plot(y,type="l")
pred <- predict(fit, inputs[-test])
lines(pred,col = "red")
#Ahora gracias al efecto memoria vamos a adelantarle a la serie al menos en un valor con una precision muy buena. Para ello volveremos a introducir los datos de entrenamiento.
predictions <- predict(fit,inputs[-train])
predictions
mod1 <- predictions*(max(Z)-min(Z))+min(Z)
mod1
x <- 1:(tamano_total+length(mod1))
y <- c(as.vector(Z),mod1)
plot(x[1:tamano_total], y[1:tamano_total],col = "blue", type="l")
lines( x[(tamano_total):length(x)], y[(tamano_total):length(x)], col="red")
mse <- mean((y_test - predictions)^2)
mse <- mean((test - predictions)^2)
# Cargar librerías necesarias
library(zoo)
library(quantmod)
library(Metrics)
install.packages("Metrics")
# Cargar librerías necesarias
library(zoo)
library(quantmod)
library(Metrics)
library(RSNNS)
install.packages("RSNNS")
# Cargar librerías necesarias
library(zoo)
library(quantmod)
library(Metrics)
library(RSNNS)
# Leer los datos
data_sitio <- read.csv("data_sitio.csv", sep = ";")
# Convertir la columna M_FECHA_DIA a tipo Date
data_sitio$M_FECHA_DIA <- as.Date(data_sitio$M_FECHA_DIA)
# Ordenar los datos por fecha
data_sitio <- data_sitio[order(data_sitio$M_FECHA_DIA), ]
# Seleccionar las columnas relevantes
data <- data_sitio[, c("M_FECHA_DIA", "PAYLOAD_4G")]
# Crear un objeto de serie temporal
# Asumiendo que los datos son diarios y empiezan en la primera fecha del dataset
data_ts <- ts(data$PAYLOAD_4G, start = c(as.numeric(format(min(data$M_FECHA_DIA), "%Y")), as.numeric(format(min(data$M_FECHA_DIA), "%j"))), frequency = 365)
# Mostrar el objeto de serie temporal
print(data_ts)
# Normalizar la serie temporal
Z <- as.ts(data_ts, F)
S <- (Z - min(Z)) / (max(Z) - min(Z))
plot(S)
# Dividir los datos en conjuntos de entrenamiento y prueba
tamano_total <- length(S)
tamano_train <- round(tamano_total * 0.75)
train <- 1:tamano_train
test <- (tamano_train + 1):tamano_total
# Crear un dataframe con n columnas para la red neuronal
y <- as.zoo(S)
lags <- 12
inputs <- do.call(cbind, lapply(1:lags, function(k) Lag(y, k)))
slogN <- cbind(y, inputs)
slogN1 <- slogN[-(1:lags), ]
DT::datatable(slogN1)
# Cargar librerías necesarias
library(zoo)
library(quantmod)
library(Metrics)
library(RSNNS)
# Leer los datos
data_sitio <- read.csv("data_sitio.csv", sep = ";")
# Convertir la columna M_FECHA_DIA a tipo Date
data_sitio$M_FECHA_DIA <- as.Date(data_sitio$M_FECHA_DIA)
# Ordenar los datos por fecha
data_sitio <- data_sitio[order(data_sitio$M_FECHA_DIA), ]
# Seleccionar las columnas relevantes
data <- data_sitio[, c("M_FECHA_DIA", "PAYLOAD_4G")]
# Crear un objeto de serie temporal
# Asumiendo que los datos son diarios y empiezan en la primera fecha del dataset
data_ts <- ts(data$PAYLOAD_4G, start = c(as.numeric(format(min(data$M_FECHA_DIA), "%Y")), as.numeric(format(min(data$M_FECHA_DIA), "%j"))), frequency = 365)
# Mostrar el objeto de serie temporal
print(data_ts)
# Mostrar el objeto de serie temporal
print(head(data_ts,5)
# Cargar librerías necesarias
library(zoo)
# Cargar librerías necesarias
library(zoo)
library(quantmod)
library(Metrics)
library(RSNNS)
# Leer los datos
data_sitio <- read.csv("data_sitio.csv", sep = ";")
# Convertir la columna M_FECHA_DIA a tipo Date
data_sitio$M_FECHA_DIA <- as.Date(data_sitio$M_FECHA_DIA)
# Ordenar los datos por fecha
data_sitio <- data_sitio[order(data_sitio$M_FECHA_DIA), ]
# Seleccionar las columnas relevantes
data <- data_sitio[, c("M_FECHA_DIA", "PAYLOAD_4G")]
# Crear un objeto de serie temporal
# Asumiendo que los datos son diarios y empiezan en la primera fecha del dataset
data_ts <- ts(data$PAYLOAD_4G, start = c(as.numeric(format(min(data$M_FECHA_DIA), "%Y")), as.numeric(format(min(data$M_FECHA_DIA), "%j"))), frequency = 365)
# Mostrar el objeto de serie temporal
head(data_ts,5)
# Mostrar el objeto de serie temporal
head(data_ts,30)
# Normalizar la serie temporal
Z <- as.ts(data_ts, F)
S <- (Z - min(Z)) / (max(Z) - min(Z))
plot(S)
# Dividir los datos en conjuntos de entrenamiento y prueba
tamano_total <- length(S)
tamano_total
tamano_train <- round(tamano_total * 0.75)
tamano_train
train <- 1:tamano_train
test <- (tamano_train + 1):tamano_total
test
s
# Normalizar la serie temporal
Z <- as.ts(data_ts, F)
S <- (Z - min(Z)) / (max(Z) - min(Z))
plot(S)
S
# Normalizar la serie temporal
Z <- as.ts(data_ts, F)
S <- (Z - min(Z)) / (max(Z) - min(Z))
head(S,100)
plot(S)
test <- (tamano_train + 1):tamano_total
test
sample(c(1:9), 10, replace = T)
sample(c(1:5), 10, replace = T)
sample(c(1:5), 6, replace = T)
sample(c(1:5), 6)
sample(c(1:5), 6, replace = T)
sample(c(1:5), 6, replace = T)
sample(c(1:5), 6, replace = T)
sample(c(1:5), 6, replace = F)
sample(c(1:5), 6, replace = FALSE)
sample(c(1:5), 6, replace != T)
sample(c(1:5), 6, replace = F)
sample(c(1:5), 6, replace = FALSE)
sample(c(1:5), 10, replace = FALSE)
sample(c(1:5), 10, replace = FALSE)
sample(c(1:9), 10, replace = FALSE)
x <- c(1, 2, 3)
y <- c("G1", "G2", "G3")
rbind(x,y)
cbind(x,y)
a <- list(A = c(8, 9, 7, 5),
B = data.frame(x = 1:5, y = c(5, 1, 0, 2, 3)))
a
a
lapply(a,sum)
b <- c(12, 18, 6)
lapply(b,sqrt)
b <- c(16, 9, 81)
lapply(b,sqrt)
c <- list(A=c(56,12,57,24),B=c(89,12,64,18,65,76) )
c
lapply(c,sum)
lapply(c,quantile)
lapply(c,percent_bias)
lapply(c,percent)
lapply(c,quantile,probs=c(0.25,0.5))
d <- 1:5
d
d <- 1:3
fun <-  function(x) { x^{2}}
lapply(d, fun)
lapply(d, xun= funcion(x) x^3)
lapply(d, FUM= funcion(x) x^3)
lapply(d, FUN =  funcion(x) x^3)
lapply(d, FUN =  function(x) x ^ 3 )
lapply(d, XUN =  function(x) x ^ 3 )
lapply(d, FUN =  function(x) x ^ 3 )
lapply(d, FUN =  function(x) x ^ 2 )
lapply(d,  function(x) x ^ 2 )
X <- vector("list",5)
x
vec <- 1:5
vec
for (i in vec) {}
df <- data.frame(x = c(6, 2), y = c(3, 6), z = c(2, 3))
ncol(df)
df
lapply(1:ncol(df), function(i) df[, i] * i)
lapply(c(1, 3), function(i) df[, i] * i)
df <- data.frame(x = c(6, 2), y = c(3, 6))
df
res <- vector("list",2)
res
for(i in 1:ncol(df)) {
for (j in 1:nrow(df)) {
res[[j]][i] <- df[j, i] * 4
}
}
res
